### **README.md**Этот проект представляет собой пайплайн для обработки данных с использованием фреймворка **Luigi**. Пайплайн выполняет следующие шаги:1. Скачивание архива с данными.2. Распаковка архива.3. Разделение таблиц на отдельные файлы.4. Удаление ненужных колонок.5. Очистка исходных файлов.---### **Шаг 0: Подготовка среды**1. **Установка необходимых библиотек**:   - Установите Python (рекомендуется версия 3.8 или выше).   - Установите Luigi и другие зависимости с помощью команды:     ```bash     pip install luigi pandas wget     ```2. **Создание структуры проекта**:   - Создайте папку для проекта, например, `luigi_pipeline`.   - Внутри папки создайте файл `pipeline.py` — это будет основной файл с кодом пайплайна.   - Создайте папку `data` для хранения скачанных и обработанных данных.---### **Шаг 1: Скачивание архива**1. **Создание задачи Luigi для скачивания**:   - В файле `pipeline.py` создайте задачу `DownloadDataset`, которая будет скачивать архив с данными.   - Используйте библиотеку `wget` для скачивания.   - Пример кода:     ```python     import luigi     import wget     import os     class DownloadDataset(luigi.Task):         dataset_name = luigi.Parameter(default="GSE68849")  # Параметр для имени датасета         def output(self):             return luigi.LocalTarget(f"data/{self.dataset_name}_RAW.tar")         def run(self):             os.makedirs("data", exist_ok=True)  # Создаем папку, если ее нет             url = f"https://www.ncbi.nlm.nih.gov/geo/download/?acc={self.dataset_name}&format=file"             wget.download(url, out=self.output().path)     ```2. **Запуск задачи**:   - Запустите задачу через командную строку:     ```bash     python -m luigi --module luigi_pipeline.pipeline DownloadDataset --local-scheduler     ```---### **Шаг 2: Распаковка архива**1. **Создание задачи для распаковки**:   - Создайте задачу `ExtractArchive`, которая будет распаковывать tar-архив и создавать папки для каждого файла.   - Пример кода:     ```python     import tarfile     import os     class ExtractArchive(luigi.Task):         dataset_name = luigi.Parameter(default="GSE68849")         def requires(self):             return DownloadDataset(dataset_name=self.dataset_name)         def output(self):             return luigi.LocalTarget(f"data/{self.dataset_name}_extracted")         def run(self):             os.makedirs(self.output().path, exist_ok=True)             with tarfile.open(self.input().path, "r") as tar:                 tar.extractall(path=self.output().path)     ```2. **Запуск задачи**:   - Запустите задачу:     ```bash     python -m luigi --module luigi_pipeline.pipeline ExtractArchive --local-scheduler     ```---### **Шаг 3: Разделение таблиц**1. **Создание задачи для разделения таблиц**:   - Создайте задачу `SplitTables`, которая будет разделять текстовые файлы на отдельные tsv-таблицы.   - Пример кода:     ```python     import pandas as pd     import io     import os     class SplitTables(luigi.Task):         dataset_name = luigi.Parameter(default="GSE68849")         def requires(self):             return ExtractArchive(dataset_name=self.dataset_name)         def output(self):             return luigi.LocalTarget(f"data/{self.dataset_name}_split")         def run(self):             os.makedirs(self.output().path, exist_ok=True)             for filename in os.listdir(self.input().path):                 if filename.endswith(".txt"):                     with open(os.path.join(self.input().path, filename), "r") as f:                         dfs = {}                         write_key = None                         fio = io.StringIO()                         for line in f.readlines():                             if line.startswith('['):                                 if write_key:                                     fio.seek(0)                                     header = None if write_key == 'Heading' else 'infer'                                     dfs[write_key] = pd.read_csv(fio, sep='\t', header=header)                                 fio = io.StringIO()                                 write_key = line.strip('[]\n')                                 continue                             if write_key:                                 fio.write(line)                         fio.seek(0)                         dfs[write_key] = pd.read_csv(fio, sep='\t')                     for key, df in dfs.items():                         df.to_csv(os.path.join(self.output().path, f"{key}.tsv"), sep='\t', index=False)     ```2. **Запуск задачи**:   - Запустите задачу:     ```bash     python -m luigi --module luigi_pipeline.pipeline SplitTables --local-scheduler     ```---### **Шаг 4: Удаление ненужных колонок**1. **Создание задачи для удаления колонок**:   - Создайте задачу `RemoveColumns`, которая удалит указанные колонки из таблицы `Probes`.   - Пример кода:     ```python     import pandas as pd     import os     class RemoveColumns(luigi.Task):         dataset_name = luigi.Parameter(default="GSE68849")         def requires(self):             return SplitTables(dataset_name=self.dataset_name)         def output(self):             return luigi.LocalTarget(f"data/{self.dataset_name}_final")         def run(self):             os.makedirs(self.output().path, exist_ok=True)             probes_path = os.path.join(self.input().path, "Probes.tsv")             df = pd.read_csv(probes_path, sep='\t')             columns_to_drop = ["Definition", "Ontology_Component", "Ontology_Process", "Ontology_Function", "Synonyms", "Obsolete_Probe_Id", "Probe_Sequence"]             df.drop(columns=columns_to_drop, inplace=True)             df.to_csv(os.path.join(self.output().path, "Probes_trimmed.tsv"), sep='\t', index=False)     ```2. **Запуск задачи**:   - Запустите задачу:     ```bash     python -m luigi --module luigi_pipeline.pipeline RemoveColumns --local-scheduler     ```---### **Шаг 5: Удаление исходных файлов**1. **Создание задачи для удаления исходных файлов**:   - Создайте задачу `Cleanup`, которая удалит исходные текстовые файлы после успешного завершения всех шагов.   - Пример кода:     ```python     import os     class Cleanup(luigi.Task):         dataset_name = luigi.Parameter(default="GSE68849")         def requires(self):             return RemoveColumns(dataset_name=self.dataset_name)         def run(self):             for filename in os.listdir(self.input().path):                 if filename.endswith(".txt"):                     os.remove(os.path.join(self.input().path, filename))     ```2. **Запуск задачи**:   - Запустите задачу:     ```bash     python -m luigi --module luigi_pipeline.pipeline Cleanup --local-scheduler     ```---### **Шаг 6: Финальный пайплайн**1. **Создание финальной задачи**:   - Создайте задачу `Pipeline`, которая объединит все шаги.   - Пример кода:     ```python     class Pipeline(luigi.WrapperTask):         dataset_name = luigi.Parameter(default="GSE68849")         def requires(self):             return Cleanup(dataset_name=self.dataset_name)     ```2. **Запуск пайплайна**:   - Запустите весь пайплайн:     ```bash     python -m luigi --module luigi_pipeline.pipeline Pipeline --local-scheduler     ```---### **Шаг 7: Проверка и сдача**1. **Проверка структуры данных**:   - Убедитесь, что папки и файлы созданы корректно.   - Пример структуры:     ```     data/     ├── GSE68849_RAW.tar     ├── GSE68849_extracted/     ├── GSE68849_split/     └── GSE68849_final/     ```2. **Сдача задания**:   - Заархивируйте папку `data` и приложите к заданию.   - Приложите файл `pipeline.py`.---### **Важные замечания**1. **Запуск из подкаталога**:   - Если файл `pipeline.py` находится в подкаталоге `luigi_pipeline`, используйте команду:     ```bash     python -m luigi --module luigi_pipeline.pipeline DownloadDataset --local-scheduler     ```2. **Добавление пути в `PYTHONPATH`**:   - Если вы хотите запускать пайплайн из любой директории, добавьте путь к `luigi_pipeline` в переменную окружения `PYTHONPATH`:     ```bash     set PYTHONPATH=C:\Urfu_study\III_Development_big _data_vol2\HW_1     python -m luigi --module luigi_pipeline.pipeline DownloadDataset --local-scheduler     ```3. **Создание файла `__init__.py`**:   - Чтобы Python распознал `luigi_pipeline` как пакет, создайте в этой директории пустой файл `__init__.py`:     ```bash     echo. > C:\Urfu_study\III_Development_big _data_vol2\HW_1\luigi_pipeline\__init__.py     ```---### **Заключение**Данный проект представляет собой законченный пайплайн для обработки данных с использованием Luigi. 